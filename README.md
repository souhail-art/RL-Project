# RL-Project

# ğŸ¦– Chrome Dinosaur RL - Apprentissage par Renforcement

Ce projet met en Å“uvre des algorithmes d'apprentissage par renforcement dans un environnement personnalisÃ© inspirÃ© du cÃ©lÃ¨bre jeu **Dino Chrome** de Google. Le but est d'entraÃ®ner un agent Ã  Ã©viter les obstacles (cactus, oiseaux) en courant Ã  travers un dÃ©sert infini.

---

## ğŸ® PrÃ©sentation du Jeu

Le jeu consiste Ã  contrÃ´ler un dinosaure qui court automatiquement dans un dÃ©cor dÃ©filant. Il doit Ã©viter des obstacles en **sautant** ou en **se baissant**. Le jeu sâ€™accÃ©lÃ¨re avec le temps, rendant la tÃ¢che de plus en plus difficile.

---

## ğŸ—ï¸ Environnement d'entraÃ®nement

Lâ€™environnement a Ã©tÃ© construit avec :

- **Gymnasium** : pour la structure de lâ€™environnement RL
- **Pygame** : pour lâ€™affichage et la simulation 2D

### Actions possibles

- `0` : Rester debout
- `1` : Sauter
- `2` : Se baisser

### Observations

Lâ€™agent ne reÃ§oit pas de vecteurs, mais une **observation visuelle (image RGB)**. Pour amÃ©liorer la perception du mouvement :

- Les **K derniÃ¨res frames** sont empilÃ©es
- RedimensionnÃ©es et converties en **niveaux de gris**

---

## ğŸ§  RÃ¨gles du jeu & SystÃ¨me de rÃ©compense

| Situation                    | RÃ©compense |
| ---------------------------- | ---------- |
| Ã‰vite un obstacle            | `+1`       |
| Collision avec un obstacle   | `-1`       |
| Autres cas (course continue) | `0`        |

- Pendant l'entraÃ®nement, la collision **ne termine pas** lâ€™Ã©pisode (pour un apprentissage continu)
- En mode Ã©valuation, une collision **met fin** Ã  lâ€™Ã©pisode (comme dans le vrai jeu)

---

## ğŸ¯ Objectif de lâ€™apprentissage

Lâ€™agent apprend Ã  **maximiser la rÃ©compense cumulative**, en Ã©vitant les obstacles aussi longtemps que possible.  
La **difficultÃ© augmente** dynamiquement avec le temps (vitesse accrue).

---

## ğŸ§ª Algorithmes TestÃ©s

### ğŸ”„ PPO (Proximal Policy Optimization)

**RÃ©sultats initiaux :**

- Lâ€™agent PPO nâ€™apprenait **aucune bonne politique** (rÃ©compenses nÃ©gatives)
  ![Demo du jeu](ppo_without_optimized.gif)
  ![metrics](metrics_plot.png)
  **AmÃ©liorations apportÃ©es :**

1. ğŸ“ DÃ©duction automatique de la taille du `Linear` (Ã©vite erreurs de dimension)
2. ğŸ¨ Normalisation des images (state / 255.0)
3. ğŸš« Clipping des avantages (stabilitÃ© des updates)
4. ğŸ“Š Augmentation du poids de lâ€™entropie (0.01 â†’ 0.02) â†’ exploration
5. ğŸ”— Utilisation de `.detach()` sur `next_state` pour Ã©viter les calculs de gradients inutiles

![Demo du jeu](ppo_optimized.gif)
![metrics](results/ppo/metrics_plot.png)

---

### âš¡ A2C (Advantage Actor Critic)

**Pourquoi A2C ?**

- âœ… Simple, rapide, sans replay buffer
- âœ… Utilise un **critic** pour rÃ©duire la variance
- âœ… Bon pour des environnements visuellement complexes

**Structure :**

- **Actor** : PrÃ©dit les probabilitÃ©s des actions
- **Critic** : Estime la valeur de lâ€™Ã©tat (`V(s)`)
- Fonction de perte :
  - `Loss_actor`
  - `Loss_critic`
  - `Entropy penalty`

![Demo du jeu](a2c.gif)
![metrics](results/25-04-22-23-32/metrics_plot.png)

---

### âœ… DQN (Deep Q-Network)

![metrics](plot.png)

**Pourquoi DQN fonctionne le mieux ?**

- ğŸ§  AdaptÃ© Ã  lâ€™**espace dâ€™actions discret**
- ğŸ’¾ Utilise une **replay buffer** pour stocker les transitions et casser les corrÃ©lations temporelles
- ğŸ” IntÃ¨gre un **rÃ©seau cible** mis Ã  jour moins frÃ©quemment â†’ stabilitÃ© accrue
- ğŸ§© Moins sensible aux **hyperparamÃ¨tres**
- ğŸ” Meilleure performance dans les environnements avec **rÃ©compenses clairsemÃ©es ou bruitÃ©es**

Dans notre expÃ©rimentation, nous avons Ã©galement modifiÃ© les **paramÃ¨tres de lâ€™environnement**, notamment la **vitesse initiale** du jeu, en passant de `20` Ã  `40`, afin de voir si cela impactait positivement lâ€™apprentissage de lâ€™agent. MalgrÃ© cette modification, les performances observÃ©es sont restÃ©es relativement **similaires**, ce qui montre la **robustesse** du DQN dans diffÃ©rents contextes de difficultÃ©.

**DQN (vitesse=20)**
![Demo du jeu](dqn.gif)
**DQN (vitesse=40)**
![Demo du jeu](dqn_speed.gif)

---

## ğŸ“Š Comparaison des Algorithmes

| Algorithme | âœ… Avantages                                       | âŒ InconvÃ©nients                    |
| ---------- | -------------------------------------------------- | ----------------------------------- |
| **DQN**    | Robuste, simple, trÃ¨s adaptÃ© aux actions discrÃ¨tes | Moins adaptÃ© aux actions continues  |
| **PPO**    | AdaptÃ© aux environnements complexes                | Sensible aux hyperparamÃ¨tres        |
| **A2C**    | Rapide, simple, peu de complexitÃ©                  | Pas de replay buffer, plus instable |

---

## ğŸš€ Pour exÃ©cuter le projet

1. Cloner le dÃ©pÃ´t :

```bash
git clone https://github.com/ton-utilisateur/chrome-dino-rl.git
cd chrome-dino-rl
```

2. Installer les dÃ©pendances :

```bash
pip install -r requirements.txt
```

3. Lancer un agent :

```bash
python play.py human #Pour lancer le jeu jouable avec le calvier
python play.py ai -m dqn.pth  # Pour lancer le jeu avec DQN pour avec la vitesse initial de l'env Ã©gale Ã  20
python play.py ai -m dqnv2.pth  # Pour lancer le jeu avec DQN pour avec la vitesse initial de l'env Ã©gale Ã  40
python play.py ai -m ppo.pth # Pour lancer le jeu avec PPO non optimisÃ© pour avec la vitesse initial de l'env Ã©gale Ã  20
python play.py ai -m ppoSlowEnv.pth # Pour lancer le jeu avec PPO optimisÃ© pour avec la vitesse initial de l'env Ã©gale Ã  20
python play.py ai -m ppov2.pth # Pour lancer le jeu avec PPO optimisÃ© pour avec la vitesse initial de l'env Ã©gale Ã  40
python play.py ai -m a2c.pth  # Pour lancer le jeu avec A2C pour avec la vitesse initial de l'env Ã©gale Ã  20
python play.py ai -m a2cv2.pth  # Pour lancer le jeu avec A2C pour avec la vitesse initial de l'env Ã©gale Ã  40
```
---
**Sources:**
Lien vers l'environnement pygame : https://github.com/MaxRohowsky/chrome-dinosaur 